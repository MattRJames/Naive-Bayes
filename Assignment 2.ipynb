{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf383fce",
   "metadata": {},
   "source": [
    "<h2>Report Section</h2>\n",
    "<h3>The Data</h3>\n",
    "\n",
    "<h3>Basic Preprocessing</h3>\n",
    "\n",
    "<p>The raw data from the trg.csv contains the id, actual classification and an abstract. The abstracts have already been converted to lowercase, and punctuation removed. Lastly, the data is randomized to prevent the order from affecting evaluation. From here, the data can then be further processed to enable compatibility with Naive Bayes.</p>\n",
    "\n",
    "<p>Basic preprocessing for Naive Bayes involves creating a matrix with a count vectorizer from sk learn. The columns represent unique words, and the rows are the abstracts. The values in the matrix are filled in with 1 or 0, depending on if the abstract contains a word. Furthermore, as the matrix is created from the training data set, the test data may contain words that have yet to be seen. Hence we need to fit the test data to this model. These matrixes are then rejoined with the respective datasets to retain the relationship between the actual protein and the abstract.</p>\n",
    " \n",
    "<h3>Basic Naive Bayes</h3>\n",
    "<p>The idea behind Naive Bayes is to create a table of probabilities based on the occurrence of unique words in the abstract. For every potential prediction(A, B, E, V), a probability for a given abstract can be calculated. The prediction would be the one with the highest probability. The numerator of the probability is the number of rows where the protein matches the actual protein and has a value of 1, plus one for Laplace smoothing. The denominator is the number of rows with the matching protein plus the number of unique words. This is repeated for every row and every protein type giving a probability matrix with columns equal to unique words and rows equal to the number of proteins(4)</p>\n",
    "\n",
    "<p>Predicting the class/protein of an abstract involves summing the log2 of the probabilities. Furthermore, it doesn't change the criteria for prediction, as the class with the highest probability is still predicted.</p>\n",
    "\n",
    "<h3>Problems and Improvements</h3>\n",
    "\n",
    "<p>The first change is to remove uninformative words which don't hold any valuable information(\"a\", \"the\", \"you\",...). These words dilute the information, making predictions less accurate. Furthermore, the storage of irrelevant words is increasing the data table's width, slowing both the training and evaluation methods. As seen in the modified preprocessing code in the coding section under #change1.=,  the stop words function and max_df are used in the count vectorizer to ignore specific words and words that occur in too many rows. In addition, some smaller changes have been made, such as removing words that don't occur frequently enough and weighting probabilities based on the frequency of the word in an abstract.</p>\n",
    "\n",
    "\n",
    "<p>From the tables in the results section, a clear problem is present the model isn't predicting abstracts to be A or V. The large imbalance in classes causes this. The class imbalance lowers accuracy because it is influenced by the proportion of classes in the table. The solution is to equalize the amount of each class in the training data before it trains the probabilities. This could be achieved through over or under-sampling. In this case, oversampling the minority classes will preserve the resolution of the large dataset, maintaining accuracy. This can be achieved with the sampling method for data frames with replacement. By sampling all the classes for the size of the largest class number of examples.</p>\n",
    "\n",
    "<h3>Evaluation Method</h3>\n",
    "\n",
    "<p>The basic evaluation of the method looks at the accuracy of predictions made by the model through the portion of correct predictions and a table with the predicted and actual values. The data is split on a 75 - 25 training test split. Ideally, cross-validation would be used to measure average performance; however, the way data is handled and processed, it becomes overly complex to use. Instead, each test is run across five different randomizations and an average is taken.</p>\n",
    "\n",
    "<h3>Results</h3>  \n",
    "<p>All the results can be found in a raw form in the testing section due to size constraints </p>\n",
    "For the basic model, the main insight we can gain is what sort of training-test split should be used and the starting accuracy. From testing five different shuffles of the data and splits the average accuracy, here are the results<p>\n",
    "    \n",
    " <p>The observation is that the algorithm continues improving as we increase the size of the training set and the test set shrinks. This is the result of the algorithm being familiar with a wider range of vocabulary and being able to recognize it in the test set. Hence the more it can train on, the better for unseen results, as shown by the gradual increase in accuracy. However, for now, we'll stick with a 75-25 split for internal testing, as a small test set size may skew the evaluation. Finally, 83-85% is a good point to start making adjustments to improve.</p>\n",
    " \n",
    " <h4>Modified Model</h4>\n",
    " <p>Before the model is modified, a baseline average accuracy from a 75 - 25 training test split is 85%. The results after implementing the changes to reduce the width of the data table and the frequency weighting is an average accuracy of 86.8% with a table:</p>\n",
    "   \n",
    "\n",
    "<p>The results show a slight improvement over the baseline. However, the table in the testing section shows that the model isn't identifying abstracts for A or V and mispredicting B as E. One of the causes of this is the imbalance in classes, with A and V combined only making up 6% of the total dataset and B being just under two-thirds the size of class B. Justifying the change to equalize the number of each class in the training set. The result of this change is an increase in accuracy to 95.4%. All classes are being predicted at around 95% except class V which is approximately 85%, depending on the sample. From this point, fine-tuning parameters will be the key to getting extra accuracy.\n",
    "\n",
    "\n",
    "\n",
    "<p> Regarding Kaggle, using the above methods with a training set comprised of the entire 4000 abstract and the test set from Kaggle results in an accuracy of 97%. Before the equalization of the dataset, the highest accuracy was 89%, and before any modifications, it was in the mid 80's. These are slightly higher than the internal result because the size of the training set is larger.</p>\n",
    "\n",
    "<p> Some final comments about different improvement methods. N-grams could have been used for higher accuracy. However, they increased the run time to an unfeasible timeframe for testing. An additional addition would be using parallel processing to speed up the predictions. Lastly, a lot of the initial data from the report has been cut due to space. However, all the raw data is in the testing section, and when running the program, be aware it will take about 10 min per evaluation.\n",
    "\n",
    "<h2>Basic Naive Bayes Coding Component</h2>\n",
    "<h4>Classifying abstracts based on what protein they focus on</h4>\n",
    "\n",
    "<h4>Step 1: Data preprocessing</h4>\n",
    "<p>Firstly, we need to move the data out of the Excel spreadsheet into a data frame to operate on it. From here, I need to create a second table that stores in binary which unique words each abstract uses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83fac956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id ActualProtein                                           Abstract\n",
      "0        1             B  the 4 202 353 bp genome of the alkaliphilic ba...\n",
      "1        2             A  the complete 1751377-bp sequence of the genome...\n",
      "2        3             E  in 1992 we started assembling an ordered libra...\n",
      "3        4             E  the aim of this study is to measure human mito...\n",
      "4        5             B  the amino acid sequence of the spirulina maxim...\n",
      "...    ...           ...                                                ...\n",
      "3995  3996             E  we have isolated and characterized two diureti...\n",
      "3996  3997             E  myotonias are muscle diseases in which the fun...\n",
      "3997  3998             E  cysteine synthase o-acetylserine sulfhydrylase...\n",
      "3998  3999             E  a region of 25 nucleotides is highly conserved...\n",
      "3999  4000             B  thermoanaerobacter tengcongensis is a rod-shap...\n",
      "\n",
      "[4000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from openpyxl import load_workbook #used for writing to excel file for kaggle\n",
    "\n",
    "#extracting data from csv\n",
    "names = [\"Id\",\"ActualProtein\",\"Abstract\"]\n",
    "rawData = pd.read_csv(\"trg.csv\",header=0,names = names)\n",
    "print(rawData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "313e1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#the data has already been converted to lower case and there appears to be no puntuation.\n",
    "#so no preprocessing required for basic functionality\n",
    "#ratio is the training - split,data is the base dataframe,randomizer is the seed for the data to be randomized\n",
    "\n",
    "def basicPreprocess(ratio,data,randomizer):\n",
    "    #randomize data and split into training and test\n",
    "    randomizedData = data.sample(frac = 1,random_state = randomizer)\n",
    "    splitPoint = int(round(len(data)*ratio,0))\n",
    "    trainingData = randomizedData.iloc[:splitPoint]\n",
    "    testData = randomizedData.iloc[splitPoint:]\n",
    "    \n",
    "    #need create data frame that stores a binary record of unique words in the abstracts. Will use sk learn count vectorizer.\n",
    "    #need to convert the abstract column to an array/list to pass to the vectorizer.\n",
    "\n",
    "    trainingAbstractList = trainingData[\"Abstract\"]\n",
    "    countVectorizer = CountVectorizer(binary=True)\n",
    "    trainArray = countVectorizer.fit_transform(trainingAbstractList).toarray()\n",
    "    \n",
    "    #convert back to dataframe\n",
    "    fittedTrainingData = pd.DataFrame(data=trainArray,columns = countVectorizer.get_feature_names_out())\n",
    "    \n",
    "    \n",
    "    #fit test data to this matrix as it handles words not seen in the training data\n",
    "    \n",
    "    testArray = countVectorizer.transform(testData[\"Abstract\"]).toarray()\n",
    "    fittedTestData = pd.DataFrame(data=testArray,columns = countVectorizer.get_feature_names_out())\n",
    "    \n",
    "    #rejoin the new tables with original to retain the actual values for evaluation\n",
    "    trainingData = trainingData.drop(\"Abstract\",axis=1)\n",
    "    testData = testData.drop(\"Abstract\",axis=1)\n",
    "    trainingData = trainingData.reset_index()\n",
    "    testData = testData.reset_index()\n",
    "   \n",
    "    trainingData = pd.concat([trainingData,fittedTrainingData],axis=1)\n",
    "    testData = pd.concat([testData,fittedTestData],axis=1)\n",
    "    trainingData = trainingData.drop(\"index\",axis=1)\n",
    "    testData = testData.drop(\"index\",axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return trainingData,testData\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346083ce",
   "metadata": {},
   "source": [
    "<h4>Step 2: Naive Bayes </h4>\n",
    "<p>want to create a table of probabilities for our training set. Note that laplace smoothing will be used to prevent probability values of 0 from breaking the log function.</p>\n",
    "<p>Note that this initial implementation is prolonged as it must calculate probabilities for all 26000 rows for each type of protein. In the modified version, optimizations will have to be made</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52935c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "#takes a set fo data and creates a 2d arry holding probabilites. table will need a column for each word and a row\n",
    "#for each potential output(protein).\n",
    "def trainProbabilities(trainingData):\n",
    "    \n",
    "    probabilities = []\n",
    "    types = [\"A\",\"B\",\"E\",\"V\"]\n",
    "    columnNames = list(trainingData.columns)\n",
    "    for protein in types:\n",
    "        probProtein = []\n",
    "        #can shift denominator out as it will be the same for all values for the same protein\n",
    "        #denominator is number of rows where the protein specified is present plus the number of unique words less two to account \n",
    "        #for columns not holding words\n",
    "        denominator = len(trainingData[(trainingData[\"ActualProtein\"] ==protein)])+len(trainingData.columns)\n",
    "\n",
    "        for i in range(2,len(trainingData.columns)):\n",
    "            \n",
    "            #numerator is number of rows with value with the given protein specified + 1 for laplace smoothing.\n",
    "            numerator = len(trainingData[(trainingData[\"ActualProtein\"] == protein) & (trainingData[columnNames[i]] != 0)])+1\n",
    "            probProtein.append(numerator/denominator)\n",
    "\n",
    "\n",
    "        probabilities.append(probProtein)\n",
    "\n",
    "    \n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c0efa2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "#takes an abstract that has been vectorised as in the preprocessing step and returns the predicted protein type using the table\n",
    "def predict(row,trainingData,probabilities):\n",
    "    #calculate probabilty of abstract being each protein and return the protein with highest probability\n",
    "    #probabilty is sum of the log of relevant probabilities plus the log of overall probabilty of the protein\n",
    "    #calculate probaility of protein in occuring in table\n",
    "    \n",
    "    #A\n",
    "    probA = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"A\")])/len(trainingData))\n",
    "    probB = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"B\")])/len(trainingData))\n",
    "    probE = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"E\")])/len(trainingData))\n",
    "    probV = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"V\")])/len(trainingData))\n",
    "    \n",
    "    for i in range(2,len(row)):\n",
    "        #probability table doesn't include rows for index and actual protein hence it is offset by 2 which must be accounted for\n",
    "        probIndex = i -2\n",
    "        \n",
    "        if row[i] == 0:\n",
    "            probA = probA + np.log2(1-(probabilities[0][probIndex]))\n",
    "            probB = probB + np.log2(1-(probabilities[1][probIndex]))\n",
    "            probE = probE + np.log2(1-(probabilities[2][probIndex]))\n",
    "            probV = probV + np.log2(1-(probabilities[3][probIndex]))\n",
    "            \n",
    "        else:\n",
    "            #change 2 multiply probability by frequency, not we can get away with using the originl predict\n",
    "            #as the binary matrix from before will multiply by 1 and have no change.\n",
    "            probA = probA + np.log2((row[i]*probabilities[0][probIndex]))\n",
    "            probB = probB + np.log2((row[i]*probabilities[1][probIndex]))\n",
    "            probE = probE + np.log2((row[i]*probabilities[2][probIndex]))\n",
    "            probV = probV + np.log2((row[i]*probabilities[3][probIndex]))\n",
    "        \n",
    "        \n",
    "\n",
    "    maxProb = max(probA,probB,probE,probV)\n",
    "    \n",
    "    if maxProb == probA:\n",
    "        return \"A\",row[1]\n",
    "    if maxProb == probB:\n",
    "        return \"B\",row[1]\n",
    "    if maxProb == probE:\n",
    "        return \"E\",row[1]\n",
    "    if maxProb == probV:\n",
    "        return \"V\",row[1]\n",
    "\n",
    "#predict values for test data and compare to actual values and return the proportion predicted correctly\n",
    "def internalEvaluate(probabilities,trainingData,testData):\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    predictionTable = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "    for row in range(0,len(testData)):\n",
    "        \n",
    "        predicted,actual = predict(testData.iloc[row],trainingData,probabilities)\n",
    "        if predicted == actual:\n",
    "                correct += 1   \n",
    "\n",
    "\n",
    "        if actual == \"A\":\n",
    "            column = 0\n",
    "        elif actual == \"B\":\n",
    "            column = 1\n",
    "        elif actual == \"E\":\n",
    "            column = 2\n",
    "        elif actual == \"V\":\n",
    "            column = 3\n",
    "\n",
    "        if predicted == \"A\":\n",
    "            row = 0\n",
    "        elif predicted == \"B\":\n",
    "            row = 1\n",
    "        elif predicted == \"E\":\n",
    "            row = 2\n",
    "        elif predicted == \"V\":\n",
    "            row = 3\n",
    "        predictionTable[row][column] = (predictionTable[row][column]) + 1 \n",
    "        \n",
    "    \n",
    "    return correct/len(testData),predictionTable\n",
    "        \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e024c29",
   "metadata": {},
   "source": [
    "<h3>Modified preprocessing</h3>\n",
    "<p>The initial data collection from the .csv is perfectly fine. However, improvements could be made to the preprocessing stage. Modify the data tables to allow for improved training and prediction. Any changes to the code will be mentioned in the surrounding comments</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "59d563a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifiedPreprocess(ratio,data,randomizer):\n",
    "    \n",
    "    randomizedData = data.sample(frac = 1,random_state = randomizer)\n",
    "    splitPoint = int(round(len(data)*ratio,0))\n",
    "    trainingData = randomizedData.iloc[:splitPoint]\n",
    "    testData = randomizedData.iloc[splitPoint:]\n",
    "    \n",
    "    #need create data frame that stores a binary record of unique words in the abstracts. Will use sk learn count vectorizer.\n",
    "    #need to convert the abstract column to an array/list to pass to the vectorizer.\n",
    "\n",
    "    trainingAbstractList = trainingData[\"Abstract\"]\n",
    "    #CHANGE #1\n",
    "    #Change count vectoriser has a parameter that allows you to insert a list of words that will be ignored\n",
    "    #reading through a few abstracts manually at rondom i picked out a selection of irrelevant words\n",
    "    ignoredWords = [\"a\",\"the\",\"an\",\"you\",\"your\",\"my\",\"our\",\"who\",\"what\",\"when\",\"why\",\"than\",\"then\",\"that\",\"youre\",\"once\",\"even\",\"though\",\"and\",\"because\",\"human\",\"test\",\"testing\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"research\",\"often\",\"some\",\"few\",\"other\"]\n",
    "    countVectorizer = CountVectorizer(min_df = 1,max_df = 0.7, stop_words = ignoredWords)\n",
    "    trainArray = countVectorizer.fit_transform(trainingAbstractList).toarray()\n",
    "    \n",
    "    #convert back to dataframe\n",
    "    fittedTrainingData = pd.DataFrame(data=trainArray,columns = countVectorizer.get_feature_names_out())\n",
    "    \n",
    "    \n",
    "    #fit test data to this matrix as it handles words not seen in the training data\n",
    "    \n",
    "    testArray = countVectorizer.transform(testData[\"Abstract\"]).toarray()\n",
    "    fittedTestData = pd.DataFrame(data=testArray,columns = countVectorizer.get_feature_names_out())\n",
    "    \n",
    "    #rejoin the new tables with original to retain the actual values for evaluation\n",
    "    trainingData = trainingData.drop(\"Abstract\",axis=1)\n",
    "    testData = testData.drop(\"Abstract\",axis=1)\n",
    "    trainingData = trainingData.reset_index()\n",
    "    testData = testData.reset_index()\n",
    "   \n",
    "    trainingData = pd.concat([trainingData,fittedTrainingData],axis=1)\n",
    "    testData = pd.concat([testData,fittedTestData],axis=1)\n",
    "    trainingData = trainingData.drop(\"index\",axis=1)\n",
    "    testData = testData.drop(\"index\",axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return trainingData,testData\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35f0b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataEqualisation(data,r):\n",
    "    #given a dataset want to equalize the quantity of each class within reason, won't be perfectly equal but close enough will do\n",
    "    dataA = data.loc[data[\"ActualProtein\"] == \"A\"]\n",
    "    dataB = data.loc[data[\"ActualProtein\"] == \"B\"]\n",
    "    dataE = data.loc[data[\"ActualProtein\"] == \"E\"]\n",
    "    dataV = data.loc[data[\"ActualProtein\"] == \"V\"]\n",
    "    \n",
    "    #want to keep resolution of largest dataset so instead want to oversmaple minority classes\n",
    "    maxDataSize = max(len(dataA),len(dataB),len(dataB),len(dataB))\n",
    "    \n",
    "    \n",
    "    \n",
    "    upsizedDataA = dataA.sample(n=maxDataSize,replace=True,random_state = r)    \n",
    "    upsizedDataB = dataB.sample(n=maxDataSize,replace=True,random_state = r)\n",
    "    upsizedDataE = dataE.sample(n=maxDataSize,replace=True,random_state = r)\n",
    "    upsizedDataV = dataV.sample(n=maxDataSize,replace=True,random_state = r)\n",
    "        \n",
    "   \n",
    "        \n",
    "    equalizedData = pd.concat([upsizedDataA,upsizedDataB,upsizedDataE,upsizedDataV])\n",
    "    return equalizedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6655de6",
   "metadata": {},
   "source": [
    "<h4>Kaggle Versions</h4>\n",
    "<p>\n",
    "as the Kaggle submission requires an Excel spreadsheet with the predictions for the unseen abstract, it requires some custom preprocess and evaluation steps to format it correctly. These functions will be functionally identical to those above; it just handles the Excel documents better.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3f6cae4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle preprocess start\n",
      "Kaggle preprocess complete\n",
      "Kaggle evaluate start\n",
      "Kaggle evaluate complete\n"
     ]
    }
   ],
   "source": [
    "def kagglePreprocess(trainingData,testData): # removes the split and randomisation part\n",
    "    print(\"Kaggle preprocess start\")\n",
    "    trainingAbstractList = trainingData[\"Abstract\"]\n",
    "    #Change count vectoriser has a parameter that allows you to insert a list of words that will be ignored\n",
    "    #reading through a few abstracts manually at rondom i picked out a selection of irrelevant words\n",
    "    ignoredWords = [\"a\",\"the\",\"an\",\"you\",\"your\",\"my\",\"our\",\"who\",\"what\",\"when\",\"why\",\"than\",\"then\",\"that\",\"youre\",\"once\",\"even\",\"though\",\"and\",\"because\",\"human\",\"test\",\"testing\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"research\",\"often\",\"some\",\"few\",\"other\"]\n",
    "    \n",
    "    countVectorizer = CountVectorizer(min_df = 1,max_df = 0.7,stop_words = ignoredWords)\n",
    "    trainArray = countVectorizer.fit_transform(trainingAbstractList).toarray()\n",
    "    \n",
    "    #convert back to dataframe\n",
    "    fittedTrainingData = pd.DataFrame(data=trainArray,columns = countVectorizer.get_feature_names_out())\n",
    "    \n",
    "    \n",
    "    #fit test data to this matrix as it handles words not seen in the training data\n",
    "    \n",
    "    testArray = countVectorizer.transform(testData[\"Abstract\"]).toarray()\n",
    "    fittedTestData = pd.DataFrame(data=testArray,columns = countVectorizer.get_feature_names_out())\n",
    "    \n",
    "    #rejoin the new tables with original to retain the actual values for evaluation\n",
    "    trainingData = trainingData.drop(\"Abstract\",axis=1)\n",
    "    testData = testData.drop(\"Abstract\",axis=1)\n",
    "    trainingData = trainingData.reset_index()\n",
    "    testData = testData.reset_index()\n",
    "   \n",
    "    trainingData = pd.concat([trainingData,fittedTrainingData],axis=1)\n",
    "    testData = pd.concat([testData,fittedTestData],axis=1)\n",
    "    trainingData = trainingData.drop(\"index\",axis=1)\n",
    "    testData = testData.drop(\"index\",axis=1)\n",
    "    \n",
    "    #remove numbers from columns\n",
    "    columnNames = testData.columns\n",
    "    remove = []\n",
    "    for word in columnNames:\n",
    "        \n",
    "        if word.isnumeric() == True:\n",
    "            remove.append(word)\n",
    "    \n",
    "    trainingData = trainingData.drop(remove,axis=1)\n",
    "    testData = testData.drop(remove,axis=1)\n",
    "    \n",
    "    \n",
    "    print(\"Kaggle preprocess complete\")\n",
    "    return trainingData,testData\n",
    "    \n",
    "\n",
    "def kaggleEvaluate(testData,trainingData,probabilities):\n",
    "    print(\"Kaggle evaluate start\")\n",
    "    predictions = []\n",
    "    for row in range(0,len(testData)):\n",
    "        \n",
    "        prediction = kagglePredict(testData.iloc[row],trainingData,probabilities)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    #put predictions into dataframe to then convert to csv\n",
    "    predictionsDF = pd.DataFrame(predictions,columns=[\"Predictions\"])\n",
    "    predictionsDF.to_csv(\"Results.csv\")\n",
    "    print(\"Kaggle evaluate complete\")\n",
    "    return predictions \n",
    "\n",
    "def kagglePredict(row,trainingData,probabilities):\n",
    "    probA = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"A\")])/len(trainingData))\n",
    "    probB = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"B\")])/len(trainingData))\n",
    "    probE = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"E\")])/len(trainingData))\n",
    "    probV = np.log2(len(trainingData[(trainingData[\"ActualProtein\"] ==\"V\")])/len(trainingData))\n",
    "    \n",
    "    for i in range(1,len(row)):\n",
    "        #probability table doesn't include rows for index and actual protein hence it is offset by 2 which must be accounted for\n",
    "        probIndex = i -1\n",
    "        \n",
    "        if row[i] == 0:\n",
    "            probA = probA + np.log2(1-(probabilities[0][probIndex]))\n",
    "            probB = probB + np.log2(1-(probabilities[1][probIndex]))\n",
    "            probE = probE + np.log2(1-(probabilities[2][probIndex]))\n",
    "            probV = probV + np.log2(1-(probabilities[3][probIndex]))\n",
    "            \n",
    "        else:\n",
    "            #change 2 multiply probability by frequency, not we can get away with using the originl predict\n",
    "            #as the binary matrix from before will multiply by 1 and have no change.\n",
    "            probA = probA + np.log2((row[i]*probabilities[0][probIndex]))\n",
    "            probB = probB + np.log2((row[i]*probabilities[1][probIndex]))\n",
    "            probE = probE + np.log2((row[i]*probabilities[2][probIndex]))\n",
    "            probV = probV + np.log2((row[i]*1.2*probabilities[3][probIndex]))\n",
    "        \n",
    "        \n",
    "\n",
    "    maxProb = max(probA,probB,probE,probV)\n",
    "    \n",
    "    if maxProb == probA:\n",
    "        return \"A\"\n",
    "    if maxProb == probB:\n",
    "        return \"B\"\n",
    "    if maxProb == probE:\n",
    "        return \"E\"\n",
    "    if maxProb == probV:\n",
    "        return \"V\"\n",
    "\n",
    "columnNames = [\"Id\",\"ActualProtein\",\"Abstract\"]\n",
    "trainData = pd.read_csv(\"trg.csv\",header=0,names = columnNames)\n",
    "columnNames = [\"Id\",\"Abstract\"]\n",
    "testData = pd.read_csv(\"tst.csv\",header=0,names = columnNames)\n",
    "\n",
    "trainData,testData = kagglePreprocess(trainData,testData)\n",
    "probs = trainProbabilities(dataEqualisation(trainData,31))\n",
    "predictions = kaggleEvaluate(testData,trainData,probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd9c1a",
   "metadata": {},
   "source": [
    "<h3>Testing</h3>\n",
    "<p>The purpose of this section is to demonstrate the figures mentioned in the report at the beginning as well as show additional testing completed during the development process.</p>\n",
    "<h4>Baseline</h4>\n",
    "<p> testing the unmodified naive Bayes algorithm, preprocessing and evaluation components. The only variable that can be modified here is the training test split. For testing, we'll use various randomizer values and take an average accuracy measure at each split ratio.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac609749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracies when the split value is 0.5\n",
      "r = 100: accuracy = 0.8305\n",
      "r = 200: accuracy = 0.848\n",
      "r = 300: accuracy = 0.8275\n",
      "r = 400: accuracy = 0.8185\n",
      "r = 500: accuracy = 0.8605\n",
      "The average accuracy when the training split ratio = 0.5 is 0.8370000000000001\n",
      "\n",
      "The accuracies when the split value is 0.6\n",
      "r = 100: accuracy = 0.840625\n",
      "r = 200: accuracy = 0.844375\n",
      "r = 300: accuracy = 0.83125\n",
      "r = 400: accuracy = 0.824375\n",
      "r = 500: accuracy = 0.858125\n",
      "The average accuracy when the training split ratio = 0.6 is 0.8397500000000001\n",
      "\n",
      "The accuracies when the split value is 0.7\n",
      "r = 100: accuracy = 0.8433333333333334\n",
      "r = 200: accuracy = 0.8583333333333333\n",
      "r = 300: accuracy = 0.8358333333333333\n",
      "r = 400: accuracy = 0.8183333333333334\n",
      "r = 500: accuracy = 0.8583333333333333\n",
      "The average accuracy when the training split ratio = 0.7 is 0.8428333333333333\n",
      "\n",
      "The accuracies when the split value is 0.8\n",
      "r = 100: accuracy = 0.85\n",
      "r = 200: accuracy = 0.855\n",
      "r = 300: accuracy = 0.85375\n",
      "r = 400: accuracy = 0.8325\n",
      "r = 500: accuracy = 0.8525\n",
      "The average accuracy when the training split ratio = 0.8 is 0.8487500000000001\n",
      "\n",
      "The accuracies when the split value is 0.9\n",
      "r = 100: accuracy = 0.855\n",
      "r = 200: accuracy = 0.855\n",
      "r = 300: accuracy = 0.8575\n",
      "r = 400: accuracy = 0.845\n",
      "r = 500: accuracy = 0.8525\n",
      "The average accuracy when the training split ratio = 0.9 is 0.853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomizerValues = [100,200,300,400,500]\n",
    "splitratios = [0.5,0.6,0.7,0.8,0.9]\n",
    "#go through all the ratios\n",
    "for ratio in splitratios:\n",
    "    print(\"The accuracies when the split value is \" + str(ratio))\n",
    "    total = 0\n",
    "    #go through all the randomizer values with that ratio\n",
    "    for r in randomizerValues:\n",
    "        trainData,testData = basicPreprocess(ratio,rawData,r)\n",
    "        probabilities = trainProbabilities(trainData)\n",
    "        accuracy,table = internalEvaluate(probabilities,trainData,testData)\n",
    "        print(\"r = \"+ str(r) + \": accuracy = \"+ str(accuracy))\n",
    "        total = total + accuracy\n",
    "    \n",
    "    print(\"The average accuracy when the training split ratio = \"+str(ratio) + \" is \"+str(total/len(randomizerValues)))\n",
    "    print()#adding a blank line for seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662c3f6",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "        <tr>\n",
    "            <td>Training Portion</td>\n",
    "            <td>Average accuracy</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>0.5</td>\n",
    "            <td>0.837</td>\n",
    "        </tr>\n",
    "            <td>0.6</td>\n",
    "            <td>0.840</td>\n",
    "        <tr>\n",
    "            <td>0.7</td>\n",
    "            <td>0.843</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>0.8</td>\n",
    "            <td>0.849</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>0.9</td>\n",
    "            <td>0.853</td>\n",
    "        </tr>\n",
    "     </table>\n",
    "<h4>Basic model on a 75 - 25 split</h4>\n",
    "<p> for the tables the columns are the actual values (A,B,E,V) and the rows are the predicted values (A,B,E,V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "45ffcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic: r = 100: accuracy = 0.851\n",
      "[0, 0, 0, 0]\n",
      "[18, 298, 0, 1]\n",
      "[13, 85, 553, 32]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "basic: r = 200: accuracy = 0.847\n",
      "[0, 0, 0, 0]\n",
      "[13, 296, 0, 0]\n",
      "[8, 98, 551, 34]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "basic: r = 300: accuracy = 0.848\n",
      "[0, 0, 0, 0]\n",
      "[17, 293, 0, 1]\n",
      "[13, 95, 555, 26]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "basic: r = 400: accuracy = 0.827\n",
      "[0, 0, 0, 0]\n",
      "[15, 291, 1, 1]\n",
      "[14, 104, 536, 38]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "basic: r = 500: accuracy = 0.852\n",
      "[0, 0, 0, 0]\n",
      "[21, 299, 0, 2]\n",
      "[15, 73, 553, 37]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "The average accuracy for the basic preprocessing is 0.845\n"
     ]
    }
   ],
   "source": [
    "randomizerValues = [100,200,300,400,500]\n",
    "ratio = 0.75\n",
    "totalbasic = 0\n",
    "\n",
    "#go through all the randomizer values with that ratio\n",
    "for r in randomizerValues:\n",
    "    #label variables associated with basic methods with a B\n",
    "    trainDataB,testDataB = basicPreprocess(ratio,rawData,r)\n",
    "    probabilitiesB = trainProbabilities(trainDataB)\n",
    "    accuracyB,tableB = internalEvaluate(probabilitiesB,trainDataB,testDataB)\n",
    "    print(\"basic: r = \"+ str(r) + \": accuracy = \"+ str(accuracyB))\n",
    "    for row in tableB:\n",
    "        print(row)\n",
    "    print()\n",
    "    totalbasic = totalbasic + accuracyB\n",
    "    \n",
    "print(\"The average accuracy for the basic preprocessing is \"+str(totalbasic/len(randomizerValues)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924de687",
   "metadata": {},
   "source": [
    "<h4>Removing oversused words, underused words and weighting by frequency</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a16262ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified: r = 100: accuracy = 0.872\n",
      "[0, 0, 0, 0]\n",
      "[24, 319, 0, 1]\n",
      "[7, 64, 553, 32]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "modified: r = 200: accuracy = 0.876\n",
      "[0, 0, 0, 0]\n",
      "[16, 326, 1, 0]\n",
      "[5, 68, 550, 34]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "modified: r = 300: accuracy = 0.871\n",
      "[0, 0, 0, 0]\n",
      "[20, 317, 1, 1]\n",
      "[10, 71, 554, 26]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "modified: r = 400: accuracy = 0.847\n",
      "[0, 0, 0, 0]\n",
      "[15, 312, 2, 1]\n",
      "[14, 83, 535, 38]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "modified: r = 500: accuracy = 0.872\n",
      "[0, 0, 0, 0]\n",
      "[25, 319, 0, 3]\n",
      "[11, 53, 553, 36]\n",
      "[0, 0, 0, 0]\n",
      "\n",
      "The average accuracy for the modified preprocessing is 0.8676\n"
     ]
    }
   ],
   "source": [
    "randomizerValues = [100,200,300,400,500]\n",
    "ratio = 0.75\n",
    "totalmodified = 0\n",
    "#go through all the randomizer values with that ratio\n",
    "for r in randomizerValues:\n",
    "    #label variable assocaited with modified methods with an M\n",
    "    trainDataM,testDataM = modifiedPreprocess(ratio,rawData,r)\n",
    "    probabilitiesM = trainProbabilities(trainDataM)\n",
    "    accuracyM,tableM = internalEvaluate(probabilitiesM,trainDataM,testDataM)\n",
    "    print(\"modified: r = \"+ str(r) + \": accuracy = \"+ str(accuracyM))\n",
    "    for row in tableM:\n",
    "        print(row)\n",
    "    \n",
    "    print()\n",
    "    totalmodified = totalmodified + accuracyM\n",
    "    \n",
    "print(\"The average accuracy for the modified preprocessing is \"+str(totalmodified/len(randomizerValues)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082955df",
   "metadata": {},
   "source": [
    "<h4>Balancing the training dataset with the modifications from above</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ed896152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified: r = 100: accuracy = 0.954\n",
      "[24, 1, 0, 0]\n",
      "[5, 356, 3, 1]\n",
      "[2, 25, 546, 4]\n",
      "[0, 1, 4, 28]\n",
      "\n",
      "modified: r = 200: accuracy = 0.963\n",
      "[16, 0, 0, 0]\n",
      "[4, 377, 9, 0]\n",
      "[0, 17, 541, 5]\n",
      "[1, 0, 1, 29]\n",
      "\n",
      "modified: r = 300: accuracy = 0.96\n",
      "[25, 0, 0, 0]\n",
      "[3, 371, 9, 1]\n",
      "[2, 16, 541, 3]\n",
      "[0, 1, 5, 23]\n",
      "\n",
      "modified: r = 400: accuracy = 0.942\n",
      "[21, 2, 0, 0]\n",
      "[5, 369, 8, 2]\n",
      "[3, 23, 527, 12]\n",
      "[0, 1, 2, 25]\n",
      "\n",
      "modified: r = 500: accuracy = 0.953\n",
      "[28, 0, 0, 0]\n",
      "[4, 354, 10, 4]\n",
      "[3, 18, 541, 5]\n",
      "[1, 0, 2, 30]\n",
      "\n",
      "The average accuracy for the modified preprocessing is 0.9544\n"
     ]
    }
   ],
   "source": [
    "randomizerValues = [100,200,300,400,500]\n",
    "ratio = 0.75\n",
    "totalmodified = 0\n",
    "#go through all the randomizer values with that ratio\n",
    "for r in randomizerValues:\n",
    "    #label variables associated with basic methods with a B\n",
    "    #label variable assocaited with modified methods with an M\n",
    "    \n",
    "    trainDataM,testDataM = modifiedPreprocess(ratio,rawData,r)\n",
    "    probabilitiesM = trainProbabilities(dataEqualisation(trainDataM,r))\n",
    "    accuracyM,tableM = internalEvaluate(probabilitiesM,trainDataM,testDataM)\n",
    "    print(\"modified: r = \"+ str(r) + \": accuracy = \"+ str(accuracyM))\n",
    "    for row in tableM:\n",
    "        print(row)\n",
    "        \n",
    "    print()\n",
    "    totalmodified = totalmodified + accuracyM\n",
    "    \n",
    "print(\"The average accuracy for the modified preprocessing is \"+str(totalmodified/len(randomizerValues)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
